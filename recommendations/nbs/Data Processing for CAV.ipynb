{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[10]\") \\\n",
    "    .appName(\"CAV Data Preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"6G\") \\\n",
    "    .config(\"spark.storage.memoryFraction\", 0.2) \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '42259'),\n",
       " ('spark.storage.memoryFraction', '0.2'),\n",
       " ('spark.driver.memory', '16G'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '172.31.5.36'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1529742544301'),\n",
       " ('spark.master', 'local[10]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'CAV Data Preprocessing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Visitor_ID\", StringType(), True),\n",
    "    StructField(\"Visit Number\", IntegerType(), True),\n",
    "    StructField(\"Products\", IntegerType(), True),\n",
    "    StructField(\"Product Views\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Report20171001-20180531.csv', 'Report20170101-20170930.csv']\n",
    "\n",
    "df = spark.read.load(files[0], header=True, format=\"csv\", schema=schema).cache();\n",
    "for i in range(1, len(files)):\n",
    "    df = df.union(spark.read.load(files[0], header=True, format=\"csv\", schema=schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Visitor_ID: string (nullable = true)\n",
      " |-- Visit Number: integer (nullable = true)\n",
      " |-- Products: integer (nullable = true)\n",
      " |-- Product Views: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746073894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------+------------+--------+-------------+\n",
      "|Date           |Visitor_ID                             |Visit Number|Products|Product Views|\n",
      "+---------------+---------------------------------------+------------+--------+-------------+\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |null    |0            |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |171236  |1            |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |38005   |1            |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |39952   |2            |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |40028   |1            |\n",
      "|October 1, 2017|1000047271602565570_7816685501896371224|5           |null    |0            |\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |null    |0            |\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |27790   |1            |\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |64059   |1            |\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |64095   |0            |\n",
      "+---------------+---------------------------------------+------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df = df.withColumn('session_id', concat(col(\"Visitor_ID\"), lit(\"_\"), col(\"Visit Number\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------+------------+--------+-------------+------------------------------------------+\n",
      "|Date           |Visitor_ID                             |Visit Number|Products|Product Views|session_id                                |\n",
      "+---------------+---------------------------------------+------------+--------+-------------+------------------------------------------+\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |null    |0            |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |171236  |1            |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |38005   |1            |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |39952   |2            |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|1000046560612521877_444244119977433712 |9           |40028   |1            |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|1000047271602565570_7816685501896371224|5           |null    |0            |1000047271602565570_7816685501896371224_5 |\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |null    |0            |1000102520911731070_1646240024588457894_40|\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |27790   |1            |1000102520911731070_1646240024588457894_40|\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |64059   |1            |1000102520911731070_1646240024588457894_40|\n",
      "|October 1, 2017|1000102520911731070_1646240024588457894|40          |64095   |0            |1000102520911731070_1646240024588457894_40|\n",
      "+---------------+---------------------------------------+------------+--------+-------------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 54)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df.filter((df[\"Visitor_ID\"] == \"\") | df[\"Visitor_ID\"].isNull()).count(), df.filter((df[\"Visit Number\"] == \"\") | df[\"Visit Number\"].isNull() | isnan(df[\"Visit Number\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import isnan\n",
    "df = df.filter((df[\"Visitor_ID\"] != \"\") & df[\"Visitor_ID\"].isNotNull() & df[\"Visit Number\"].isNotNull() & ~isnan(df[\"Visit Number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746073840"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Products'].isNotNull() & (df['Product Views'] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430612176"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Visitor_ID', 'Visit Number', 'Product Views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------------------------------------------+\n",
      "|Date           |Products|session_id                                |\n",
      "+---------------+--------+------------------------------------------+\n",
      "|October 1, 2017|171236  |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|38005   |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|39952   |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|40028   |1000046560612521877_444244119977433712_9  |\n",
      "|October 1, 2017|27790   |1000102520911731070_1646240024588457894_40|\n",
      "|October 1, 2017|64059   |1000102520911731070_1646240024588457894_40|\n",
      "|October 1, 2017|88999   |1000184036754529481_6340868211800103312_34|\n",
      "|October 1, 2017|81764   |1000206142114360214_4025469838733486262_17|\n",
      "|October 1, 2017|28760   |1000211218072042403_3360638402315771367_6 |\n",
      "|October 1, 2017|63379   |1000211218072042403_3360638402315771367_6 |\n",
      "+---------------+--------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Products', 'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/nykaa/api\")\n",
    "from pas.v2.utils import Utils\n",
    "\n",
    "nykaadb = Utils.nykaaMysqlConnection()\n",
    "cursor = nykaadb.cursor()\n",
    "\n",
    "def extract_data(query):\n",
    "    cursor.execute(query)\n",
    "    rows = []\n",
    "    BATCH_SIZE = 10000\n",
    "\n",
    "    while True:\n",
    "        batch_empty = True\n",
    "        for row in cursor.fetchmany(BATCH_SIZE):\n",
    "            batch_empty = False\n",
    "            rows.append(row)\n",
    "        if batch_empty:\n",
    "            break\n",
    "            \n",
    "    return rows\n",
    "\n",
    "query = \"select child_id, parent_id from catalog_product_relation\"\n",
    "\n",
    "rows = extract_data(query)\n",
    "child_2_parent = {row[0]: row[1] for row in rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"child_product_2_parent.json\", \"r+\") as f:\n",
    "    child_2_parent.update({int(key): int(value) for key, value in json.load(f).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259561"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(child_2_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def convert_to_parent(product_id):\n",
    "    return child_2_parent.get(product_id, product_id)\n",
    "\n",
    "convert_to_parent_udf = udf(convert_to_parent, IntegerType())\n",
    "df = df.withColumn(\"product_id\", convert_to_parent_udf(df['product_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.option(\"header\", \"true\").csv('processed_cav_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
