{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[10]\") \\\n",
    "    .appName(\"CAV Data Preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"6G\") \\\n",
    "    .config(\"spark.storage.memoryFraction\", 0.2) \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1539343095352'),\n",
       " ('spark.storage.memoryFraction', '0.2'),\n",
       " ('spark.driver.memory', '16G'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '172.31.5.36'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[10]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '32848'),\n",
       " ('spark.app.name', 'CAV Data Preprocessing')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Visitor_ID\", StringType(), True),\n",
    "    StructField(\"Visit Number\", IntegerType(), True),\n",
    "    StructField(\"Products\", IntegerType(), True),\n",
    "    StructField(\"Product Views\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/home/ubuntu/data/cav/Report_Desktop_20180401-20180831.csv', '/home/ubuntu/data/cav/Report_sept.csv']\n",
    "\n",
    "df = spark.read.load(files[0], header=True, format=\"csv\", schema=schema).cache();\n",
    "for i in range(1, len(files)):\n",
    "    df = df.union(spark.read.load(files[0], header=True, format=\"csv\", schema=schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Visitor_ID: string (nullable = true)\n",
      " |-- Visit Number: integer (nullable = true)\n",
      " |-- Products: integer (nullable = true)\n",
      " |-- Product Views: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213097620"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------+------------+--------+-------------+\n",
      "|Date         |Visitor_ID                             |Visit Number|Products|Product Views|\n",
      "+-------------+---------------------------------------+------------+--------+-------------+\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |null    |0            |\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |248528  |1            |\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |27115   |1            |\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|108         |null    |0            |\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|109         |null    |0            |\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|110         |null    |0            |\n",
      "|April 1, 2018|1000151524024214239_4749141478071163189|10          |null    |0            |\n",
      "|April 1, 2018|1000151524024214239_4749141478071163189|11          |null    |0            |\n",
      "|April 1, 2018|1000176533271560783_6825180597277073644|1           |null    |0            |\n",
      "|April 1, 2018|1000192141674541491_1391190144946070955|2           |null    |0            |\n",
      "+-------------+---------------------------------------+------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df = df.withColumn('session_id', concat(col(\"Visitor_ID\"), lit(\"_\"), col(\"Visit Number\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------+------------+--------+-------------+-------------------------------------------+\n",
      "|Date         |Visitor_ID                             |Visit Number|Products|Product Views|session_id                                 |\n",
      "+-------------+---------------------------------------+------------+--------+-------------+-------------------------------------------+\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |null    |0            |1000021042451551118_7817197741417963751_2  |\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |248528  |1            |1000021042451551118_7817197741417963751_2  |\n",
      "|April 1, 2018|1000021042451551118_7817197741417963751|2           |27115   |1            |1000021042451551118_7817197741417963751_2  |\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|108         |null    |0            |1000098726092126499_5215981598137947057_108|\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|109         |null    |0            |1000098726092126499_5215981598137947057_109|\n",
      "|April 1, 2018|1000098726092126499_5215981598137947057|110         |null    |0            |1000098726092126499_5215981598137947057_110|\n",
      "|April 1, 2018|1000151524024214239_4749141478071163189|10          |null    |0            |1000151524024214239_4749141478071163189_10 |\n",
      "|April 1, 2018|1000151524024214239_4749141478071163189|11          |null    |0            |1000151524024214239_4749141478071163189_11 |\n",
      "|April 1, 2018|1000176533271560783_6825180597277073644|1           |null    |0            |1000176533271560783_6825180597277073644_1  |\n",
      "|April 1, 2018|1000192141674541491_1391190144946070955|2           |null    |0            |1000192141674541491_1391190144946070955_2  |\n",
      "+-------------+---------------------------------------+------------+--------+-------------+-------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53134, 53134)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "df.filter((df[\"Visitor_ID\"] == \"\") | df[\"Visitor_ID\"].isNull()).count(), df.filter((df[\"Visit Number\"] == \"\") | df[\"Visit Number\"].isNull() | isnan(df[\"Visit Number\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import isnan\n",
    "df = df.filter((df[\"Visitor_ID\"] != \"\") & df[\"Visitor_ID\"].isNotNull() & df[\"Visit Number\"].isNotNull() & ~isnan(df[\"Visit Number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213044486"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Products'].isNotNull() & (df['Product Views'] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105572610"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Visitor_ID', 'Visit Number', 'Product Views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------------------------------------+\n",
      "|Date         |Products|session_id                                |\n",
      "+-------------+--------+------------------------------------------+\n",
      "|April 1, 2018|248528  |1000021042451551118_7817197741417963751_2 |\n",
      "|April 1, 2018|27115   |1000021042451551118_7817197741417963751_2 |\n",
      "|April 1, 2018|216554  |1000213868797828724_8491859803852391686_6 |\n",
      "|April 1, 2018|25253   |1000220416542877232_449178730912277620_1  |\n",
      "|April 1, 2018|110277  |1000247315170957961_2306277511903815026_31|\n",
      "|April 1, 2018|110290  |1000247315170957961_2306277511903815026_31|\n",
      "|April 1, 2018|214397  |1000247315170957961_2306277511903815026_31|\n",
      "|April 1, 2018|214399  |1000247315170957961_2306277511903815026_31|\n",
      "|April 1, 2018|218996  |1000247315170957961_2306277511903815026_31|\n",
      "|April 1, 2018|219004  |1000247315170957961_2306277511903815026_31|\n",
      "+-------------+--------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Products', 'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182498"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/var/www/nykaa')\n",
    "from pas.v2.utils import Utils\n",
    "\n",
    "child_2_parent = DiscUtils.scrollESForResults()['child_2_parent']\n",
    "len(child_2_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def convert_to_parent(product_id):\n",
    "    return child_2_parent.get(product_id, product_id)\n",
    "\n",
    "convert_to_parent_udf = udf(convert_to_parent, IntegerType())\n",
    "df = df.withColumn(\"product_id\", convert_to_parent_udf(df['product_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.coalesce(1).write.option(\"header\", \"true\").csv('/home/ubuntu/data/cav/processed_cav_desktop_data_2018_04_2018_09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
